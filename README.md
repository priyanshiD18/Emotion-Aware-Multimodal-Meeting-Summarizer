# ğŸ™ï¸ Agentic AI Meeting Intelligence Platform

A production-ready AI pipeline for meeting transcription, speaker diarization, emotion detection, and intelligent summarization using multi-agent orchestration.

## ğŸŒŸ Features

- **High-Accuracy Transcription**: WhisperX for state-of-the-art ASR
- **Speaker Diarization**: Automatic speaker identification and labeling
- **Emotion Detection**: Real-time emotion and sentiment analysis per speaker
- **Multi-Agent Intelligence**: LangChain agents for action extraction, sentiment analysis, and context verification
- **Structured Outputs**: JSON-formatted summaries with decisions, action items, and speaker insights
- **Real-Time Visualization**: Interactive emotion timelines and speaker analytics
- **Production-Ready**: FastAPI backend, Streamlit frontend, Docker support

## ğŸ—ï¸ Architecture

```
audio_ML/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ audio/              # Audio processing & preprocessing
â”‚   â”œâ”€â”€ models/             # ASR, diarization, emotion detection
â”‚   â”œâ”€â”€ agents/             # Multi-agent LangChain orchestration
â”‚   â”œâ”€â”€ api/                # FastAPI backend
â”‚   â””â”€â”€ frontend/           # Streamlit UI
â”œâ”€â”€ data/                   # Data storage
â”œâ”€â”€ tests/                  # Unit & integration tests
â”œâ”€â”€ docker/                 # Docker configurations
â””â”€â”€ configs/                # Configuration files
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment configuration
cp .env.example .env
# Edit .env with your API keys
```

### 2. Model Downloads

```bash
# Download required models
python scripts/download_models.py
```

### 3. Run the Application

```bash
# Start FastAPI backend
uvicorn src.api.main:app --reload --port 8000

# In another terminal, start Streamlit frontend
streamlit run src/frontend/app.py
```

### 4. Run with Docker

```bash
# Build and start services
docker-compose up --build

# Access at:
# - API: http://localhost:8000
# - Frontend: http://localhost:8501
```

## ğŸ“Š Usage

### Upload Audio
1. Navigate to http://localhost:8501
2. Upload a meeting recording (.wav, .mp3, .m4a)
3. Click "Process Audio"

### View Results
- **Transcription**: Full transcript with speaker labels
- **Emotion Timeline**: Interactive visualization of emotions over time
- **Summary**: Structured JSON with decisions and action items
- **Analytics**: Speaker statistics and engagement metrics

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test suite
pytest tests/test_audio_processing.py -v
```

## ğŸ“ˆ MLOps & Monitoring

This project integrates Weights & Biases for experiment tracking:

```bash
# Initialize W&B
wandb login

# Experiments are automatically logged during processing
# View at: https://wandb.ai/your-entity/meeting-intelligence
```

## ğŸ¯ Performance Metrics

- **WER (Word Error Rate)**: < 10% on clean audio
- **DER (Diarization Error Rate)**: < 15%
- **Emotion F1-Score**: > 0.75
- **Processing Speed**: ~0.3x real-time on GPU

## ğŸ”§ Configuration

Key configurations in `.env`:
- **LLM Provider**: Switch between OpenAI GPT-4 or Google Gemini
- **Model Sizes**: Adjust Whisper model size for speed/accuracy tradeoff
- **Emotion Threshold**: Fine-tune emotion detection sensitivity

## ğŸš¢ Deployment

### Cloud Deployment Options

**Google Cloud Platform**:
```bash
gcloud run deploy meeting-intelligence --source .
```

**AWS Lambda + API Gateway**:
```bash
serverless deploy
```

**Azure Container Apps**:
```bash
az containerapp up --name meeting-intelligence --source .
```

## ğŸ› ï¸ Development Phases

- [x] Phase 1: Project Setup
- [ ] Phase 2: Audio Processing & Transcription
- [ ] Phase 3: Speaker Diarization & Emotion Detection
- [ ] Phase 4: LLM Orchestration & Summarization
- [ ] Phase 5: Backend API
- [ ] Phase 6: Frontend & Visualization
- [ ] Phase 7: Evaluation & Iteration
- [ ] Phase 8: Deployment

## ğŸ“ API Documentation

Once running, visit http://localhost:8000/docs for interactive API documentation.

### Key Endpoints

- `POST /api/v1/upload`: Upload audio file
- `POST /api/v1/transcribe`: Transcribe audio
- `POST /api/v1/analyze`: Full analysis pipeline
- `GET /api/v1/summary/{task_id}`: Retrieve results

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests
5. Submit a pull request

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

- OpenAI Whisper team
- Pyannote.audio developers
- SpeechBrain community
- LangChain maintainers

---

**Built with â¤ï¸ for intelligent meeting analysis**

